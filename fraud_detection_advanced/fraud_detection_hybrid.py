# -*- coding: utf-8 -*-
"""fraud_detection_hybrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E5hPrptTEQnGuXYlcj3NSOwwMGFsOQ-M
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from xgboost import XGBClassifier
import joblib
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

def prepare_data(df, scaler_type='minmax'):
    print("\n" + "="*50)
    print(f"DATA PREPARATION (Scaler: {scaler_type})")
    print("="*50)

    X = df.drop('fraud', axis=1)
    y = df['fraud']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print(f"Training set shape: {X_train_scaled.shape}")
    print(f"Test set shape: {X_test_scaled.shape}")
    print(f"Training set fraud ratio: {y_train.mean():.4f}")
    print(f"Test set fraud ratio: {y_test.mean():.4f}")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

def create_autoencoder(input_dim, encoding_dim=16, learning_rate=0.0005):
    input_layer = Input(shape=(input_dim,), name='input')

    encoded = Dense(16, name='encoder_1')(input_layer)
    encoded = LeakyReLU(alpha=0.1)(encoded)
    encoded = Dropout(0.2)(encoded)
    encoded = Dense(8, name='encoder_2')(encoded)
    encoded = LeakyReLU(alpha=0.1)(encoded)
    encoded = Dropout(0.2)(encoded)
    encoded = Dense(encoding_dim, name='encoder_bottleneck')(encoded)
    encoded = LeakyReLU(alpha=0.1)(encoded)

    decoded = Dense(8, name='decoder_1')(encoded)
    decoded = LeakyReLU(alpha=0.1)(decoded)
    decoded = Dropout(0.2)(decoded)
    decoded = Dense(16, name='decoder_2')(decoded)
    decoded = LeakyReLU(alpha=0.1)(decoded)
    decoded = Dropout(0.2)(decoded)
    decoded = Dense(input_dim, activation='sigmoid', name='decoder_output')(decoded)

    autoencoder = Model(inputs=input_layer, outputs=decoded, name='autoencoder')

    autoencoder.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='mean_squared_error',
        metrics=['mae']
    )

    return autoencoder

def train_autoencoder(autoencoder, X_train, y_train, epochs=100, batch_size=32):
    print("\n" + "="*50)
    print("TRAINING AUTOENCODER")
    print("="*50)

    normal_transactions = X_train[y_train == 0]
    print(f"Training on {len(normal_transactions)} normal transactions")
    print(f"Excluded {len(X_train) - len(normal_transactions)} fraudulent transactions")

    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)

    history = autoencoder.fit(
        normal_transactions,
        normal_transactions,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.1,
        verbose=1,
        shuffle=True,
        callbacks=[early_stopping, lr_scheduler]
    )

    return history

def evaluate_model(autoencoder, X_test, y_test, scaler):
    print("\n" + "="*50)
    print("MODEL EVALUATION")
    print("="*50)

    reconstructions = autoencoder.predict(X_test, verbose=0)
    reconstruction_errors = np.mean((X_test - reconstructions) ** 2, axis=1)
    auc_score = roc_auc_score(y_test, reconstruction_errors)

    reconstruction_errors_normalized = reconstruction_errors / np.max(reconstruction_errors)

    thresholds = np.percentile(reconstruction_errors, np.arange(80, 100, 0.5))
    best_f1, best_threshold = 0, 0
    for thresh in thresholds:
        preds = (reconstruction_errors > thresh).astype(int)
        f1 = f1_score(y_test, preds)
        if f1 > best_f1:
            best_f1, best_threshold = f1, thresh

    print(f"AUC Score: {auc_score:.4f}")
    print(f"Best Threshold: {best_threshold:.6f}")
    print(f"Best F1-Score: {f1:.4f}")
    print(f"Mean reconstruction error (normal): {reconstruction_errors[y_test == 0].mean():.6f}")
    print(f"Mean reconstruction error (fraud): {reconstruction_errors[y_test == 1].mean():.6f}")
    print(f"Normalized mean error (normal): {reconstruction_errors_normalized[y_test == 0].mean():.6f}")
    print(f"Normalized mean error (fraud): {reconstruction_errors_normalized[y_test == 1].mean():.6f}")

    return reconstruction_errors, auc_score, best_threshold, f1, reconstruction_errors_normalized

def plot_roc_curve(y_test, scores, best_threshold, best_f1, title="ROC Curve"):
    fpr, tpr, thresholds = roc_curve(y_test, scores)
    auc_value = auc(fpr, tpr)

    closest_threshold_idx = np.argmin(np.abs(thresholds - best_threshold))
    closest_fpr = fpr[closest_threshold_idx]
    closest_tpr = tpr[closest_threshold_idx]

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_value:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')
    plt.scatter([closest_fpr], [closest_tpr], color='red', s=100, label=f'Optimal Threshold (F1 = {best_f1:.3f})')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.savefig(f'/content/drive/MyDrive/roc_curve_{title.lower().replace(" ", "_")}.png')
    plt.show()

def train_supervised_model(autoencoder, X_train, X_test, y_train, y_test):
    print("\n" + "="*50)
    print("TRAINING SUPERVISED MODEL (XGBOOST)")
    print("="*50)

    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoder_bottleneck').output)
    X_train_encoded = encoder.predict(X_train, verbose=0)
    X_test_encoded = encoder.predict(X_test, verbose=0)

    clf = XGBClassifier(max_depth=3, n_estimators=100, random_state=42, eval_metric='auc')
    clf.fit(X_train_encoded, y_train)

    y_pred_proba = clf.predict_proba(X_test_encoded)[:, 1]
    auc_score = roc_auc_score(y_test, y_pred_proba)
    y_pred = clf.predict(X_test_encoded)
    f1 = f1_score(y_test, y_pred)

    cv_scores = cross_val_score(clf, X_train_encoded, y_train, cv=5, scoring='roc_auc')
    print(f"XGBoost AUC: {auc_score:.4f}")
    print(f"XGBoost F1-Score: {f1:.4f}")

    joblib.dump(clf, '/content/drive/MyDrive/xgboost_model_minmax_final_20250705_final.pkl')
    print("XGBoost model saved to /content/drive/MyDrive/xgboost_model_minmax_final_20250705_final.pkl")

    print(f"5-Fold CV AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")
    print("XGBoost Feature Importance:", clf.feature_importances_)

    return y_pred_proba, auc_score, f1, cv_scores, clf.feature_importances_

def save_summary(auc_auto, f1_auto, auc_xgb, f1_xgb, cv_scores, feature_importance, reconstruction_errors, reconstruction_errors_normalized, y_test):
    summary = f"""
Credit Card Fraud Detection Summary (2025-07-05 Final)
==================================================
Autoencoder:
- AUC: {auc_auto:.4f}
- F1-Score: {f1_auto:.4f}
- Mean Reconstruction Error (Normal): {reconstruction_errors[y_test == 0].mean():.6f}
- Mean Reconstruction Error (Fraud): {reconstruction_errors[y_test == 1].mean():.6f}
- Normalized Error (Normal): {reconstruction_errors_normalized[y_test == 0].mean():.6f}
- Normalized Error (Fraud): {reconstruction_errors_normalized[y_test == 1].mean():.6f}

XGBoost:
- AUC: {auc_xgb:.4f}
- F1-Score: {f1_xgb:.4f}
- 5-Fold CV AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}
- Feature Importance: {feature_importance.tolist()}

Interpretation:
- XGBoost AUC > 0.9: Excellent performance! The model effectively distinguishes fraud.
- Autoencoder AUC > 0.8: Strong unsupervised performance, effective as feature encoder for XGBoost.
"""
    with open('/content/drive/MyDrive/fraud_detection_summary_20250705_final.txt', 'w') as f:
        f.write(summary)
    print("Summary saved to /content/drive/MyDrive/fraud_detection_summary_20250705_final.txt")

def main():
    print("Credit Card Fraud Detection using Autoencoders")
    print("=" * 60)

    try:
        # Mount Google Drive
        from google.colab import drive
        drive.mount('/content/drive', force_remount=True)

        # Load engineered dataset
        engineered_path = '/content/drive/MyDrive/engineered_data.pkl'
        if not os.path.exists(engineered_path):
            raise FileNotFoundError(f"Engineered dataset not found at {engineered_path}. Please ensure it exists.")
        df_engineered = pd.read_pickle(engineered_path)
        print("Loaded engineered dataset:", df_engineered.shape)

        # Prepare data
        X_train, X_test, y_train, y_test, scaler = prepare_data(df_engineered, scaler_type='minmax')

        # Save scaler
        joblib.dump(scaler, '/content/drive/MyDrive/scaler_minmax_final_20250705_final.pkl')
        print("Scaler saved to /content/drive/MyDrive/scaler_minmax_final_20250705_final.pkl")

        # Train autoencoder
        autoencoder = create_autoencoder(
            input_dim=X_train.shape[1],
            encoding_dim=8,
            learning_rate=0.0005
        )
        print("\nAutoencoder Architecture:")
        autoencoder.summary()

        history = train_autoencoder(autoencoder, X_train, y_train, epochs=100, batch_size=32)

        # Save model
        autoencoder.save('/content/drive/MyDrive/autoencoder_minmax_final_20250705_final.keras')
        print("Model saved to /content/drive/MyDrive/autoencoder_minmax_final_20250705_final.keras")

        # Evaluate autoencoder
        reconstruction_errors, auc_score, best_threshold, f1_score, reconstruction_errors_normalized = evaluate_model(autoencoder, X_test, y_test, scaler)
        plot_roc_curve(y_test, reconstruction_errors, best_threshold, f1_score, title="Autoencoder MinMaxScaler Final 20250705")

        print(f"Autoencoder AUC: {auc_score:.4f}, F1: {f1_score:.4f}")

        # Train supervised model
        print("\nTraining supervised model...")
        y_pred_proba, xgb_auc, xgb_f1, cv_scores, feature_importance = train_supervised_model(autoencoder, X_train, X_test, y_train, y_test)
        plot_roc_curve(y_test, y_pred_proba, best_threshold, xgb_f1, title="XGBoost MinMaxScaler Final 20250705")

        print(f"XGBoost AUC: {xgb_auc:.4f}, F1: {xgb_f1:.4f}")

        # Save summary
        save_summary(auc_score, f1_score, xgb_auc, xgb_f1, cv_scores, feature_importance, reconstruction_errors, reconstruction_errors_normalized, y_test)

        # Final results
        print("\n" + "="*60)
        print("ANALYSIS COMPLETE")
        print("="*60)
        print(f"Final AUC Score: {xgb_auc:.4f}")
        print(f"Final F1-Score: {xgb_f1:.4f}")

        print("\nInterpretation:")
        if xgb_auc > 0.9:
            print("- Excellent performance! The model effectively distinguishes fraud.")
        elif xgb_auc > 0.8:
            print("- Good performance. The model shows strong fraud detection capability.")
        elif xgb_auc > 0.7:
            print("- Fair performance. Consider more feature engineering or model tuning.")
        else:
            print("- Poor performance. The model needs significant improvement.")

    except Exception as e:
        print(f"Error occurred: {str(e)}")
        print("Ensure dependencies are installed: pip install tensorflow pandas scikit-learn matplotlib xgboost")

if __name__ == "__main__":
    main()