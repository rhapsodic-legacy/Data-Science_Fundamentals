---

title: PPO for transformer models

keywords: fastai
sidebar: home_sidebar

summary: "A Pytorch implementation of Proximal Policy Optimization for transfomer models."
description: "A Pytorch implementation of Proximal Policy Optimization for transfomer models."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02-ppo.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This follows the language model approach proposed in paper <a href="https://arxiv.org/pdf/1909.08593.pdf">"Fine-Tuning Language Models from Human Preferences"</a> and is similar to the <a href="https://github.com/openai/lm-human-preferences">original implementation</a>. The two main differences are 1) the method is implemented in Pytorch and 2) works with the <code>transformer</code> library by Hugging Face.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="KL-controllers">KL-controllers<a class="anchor-link" href="#KL-controllers"> </a></h2><p>To ensure that the learned policy does not deviate to much from the original language model the KL divergence between the policy and a reference policy (the language model before PPO training) is used as an additional reward signal. Large KL-divergences are punished and staying close to the reference is rewarded.</p>
<p>Two controllers are presented in the paper: an adaptive log-space proportional controller and a fixed controller.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">AdaptiveKLController</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adaptive KL controller described in the paper:</span>
<span class="sd">    https://arxiv.org/pdf/1909.08593.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init_kl_coef</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">horizon</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">init_kl_coef</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">horizon</span> <span class="o">=</span> <span class="n">horizon</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
        <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span>
        <span class="n">proportional_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">current</span> <span class="o">/</span> <span class="n">target</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
        <span class="n">mult</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">proportional_error</span> <span class="o">*</span> <span class="n">n_steps</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">horizon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">*=</span> <span class="n">mult</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdaptiveKLController" class="doc_header"><code>class</code> <code>AdaptiveKLController</code><a href="https://github.com/lvwerra/trl/tree/master/trl/ppo.py#L26" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdaptiveKLController</code>(<strong><code>init_kl_coef</code></strong>, <strong><code>target</code></strong>, <strong><code>horizon</code></strong>)</p>
</blockquote>
<p>Adaptive KL controller described in the paper:
<a href="https://arxiv.org/pdf/1909.08593.pdf">https://arxiv.org/pdf/1909.08593.pdf</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">FixedKLController</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Fixed KL controller.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kl_coef</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">kl_coef</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
        <span class="k">pass</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FixedKLController" class="doc_header"><code>class</code> <code>FixedKLController</code><a href="https://github.com/lvwerra/trl/tree/master/trl/ppo.py#L44" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FixedKLController</code>(<strong><code>kl_coef</code></strong>)</p>
</blockquote>
<p>Fixed KL controller.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">PPOTrainer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The PPO_trainer uses Proximal Policy Optimization to optimise language models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">default_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1.41e-5</span><span class="p">,</span>
        <span class="s2">&quot;adap_kl_ctrl&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> 
        <span class="s2">&quot;init_kl_coef&quot;</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
        <span class="s2">&quot;horizon&quot;</span><span class="p">:</span><span class="mi">10000</span><span class="p">,</span>
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;lam&quot;</span><span class="p">:</span><span class="mf">0.95</span><span class="p">,</span>
        <span class="s2">&quot;cliprange&quot;</span><span class="p">:</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;cliprange_value&quot;</span><span class="p">:</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;vf_coef&quot;</span><span class="p">:</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;forward_batch_size&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
        <span class="s2">&quot;ppo_epochs&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>    
    <span class="p">}</span> 
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">ref_model</span><span class="p">,</span> <span class="o">**</span><span class="n">ppo_params</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize PPOTrainer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            model (torch.model): Hugging Face transformer GPT2 model with value head</span>
<span class="sd">            ref_model (torch.model): Hugging Face transformer GPT2 refrence model used for KL penalty</span>
<span class="sd">            ppo_params (dict or None): PPO parameters for training. Can include following keys:</span>
<span class="sd">                &#39;lr&#39; (float): Adam learning rate, default: 1.41e-5</span>
<span class="sd">                &#39;batch_size&#39; (int): Number of samples per optimisation step, default: 256</span>
<span class="sd">                &#39;forward_batch_size&#39; (int): Number of samples forward passed through model at a time, default: 16</span>
<span class="sd">                &#39;ppo_epochs&#39; (int): Number of optimisation epochs per batch of samples, default: 4</span>
<span class="sd">                &#39;gamma&#39; (float)): Gamma parameter for advantage calculation, default: 1.</span>
<span class="sd">                &#39;lam&#39; (float): Lambda parameter for advantage calcualation, default: 0.95</span>
<span class="sd">                &#39;cliprange_value&#39; (float): Range for clipping values in loss calculation, default: 0.2</span>
<span class="sd">                &#39;cliprange&#39; (float): Range for clipping in PPO policy gradient loss, default: 0.2</span>
<span class="sd">                &#39;vf_coef&#39; (float): Scaling factor for value loss, default: 0.1</span>
<span class="sd">                &#39;adap_kl_ctrl&#39; (bool): Use adaptive KL control, otherwise linear, default: True</span>
<span class="sd">                &#39;init_kl_coef&#39; (float): Initial KL penalty coefficient (used for adaptive and linear control), default: 0.2</span>
<span class="sd">                &#39;target&#39; (float): Target KL value for adaptive KL control, default: 6.0</span>
<span class="sd">                &#39;horizon&#39; (float): Horizon for adaptive KL control, default: 10000</span>
<span class="sd">                </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ppo_params</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span> <span class="o">=</span> <span class="n">ref_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
     
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_ctl</span> <span class="o">=</span> <span class="n">AdaptiveKLController</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;init_kl_coef&#39;</span><span class="p">],</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">],</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;horizon&#39;</span><span class="p">])</span>


    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">scores</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run a PPO optimisation step.</span>
<span class="sd">        </span>
<span class="sd">        args:</span>
<span class="sd">            query (torch.tensor): tensor containing the encoded queries, shape [batch_size, query_length]</span>
<span class="sd">            response (torch.tensor): tensor containing the encoded responses, shape [batch_size, response_length]</span>
<span class="sd">            scores (torch.tensor): tensor containing the scores, shape [batch_size]</span>
<span class="sd">            </span>
<span class="sd">        returns:</span>
<span class="sd">            train_stats (dict): a summary of the training statistics</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
        <span class="n">timing</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="n">gen_len</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">query</span><span class="p">,</span> <span class="n">response</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">logprobs</span><span class="p">,</span> <span class="n">ref_logprobs</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batched_forward_pass</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">gen_len</span><span class="p">)</span>
        <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/ppo/forward_pass&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span>

        <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">rewards</span><span class="p">,</span> <span class="n">non_score_reward</span><span class="p">,</span> <span class="n">kl_coef</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_rewards</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">ref_logprobs</span><span class="p">)</span>
        <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/ppo/compute_rewards&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span> 
        
        <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> 
        <span class="n">all_stats</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;ppo_epochs&#39;</span><span class="p">]):</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">train_stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_minibatch</span><span class="p">(</span><span class="n">logprobs</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">values</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                                                   <span class="n">rewards</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">query</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                                                   <span class="n">response</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">model_input</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">all_stats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_stats</span><span class="p">)</span>
        <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/ppo/optimize_step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span>
        
        <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">train_stats</span> <span class="o">=</span> <span class="n">stack_dicts</span><span class="p">(</span><span class="n">all_stats</span><span class="p">)</span>
        
        <span class="c1"># reshape advantages/ratios such that they are not averaged.</span>
        <span class="n">train_stats</span><span class="p">[</span><span class="s1">&#39;policy/advantages&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">train_stats</span><span class="p">[</span><span class="s1">&#39;policy/advantages&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">train_stats</span><span class="p">[</span><span class="s1">&#39;policy/ratio&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">train_stats</span><span class="p">[</span><span class="s1">&#39;policy/ratio&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_step_stats</span><span class="p">(</span><span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span> <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span> <span class="n">ref_logprobs</span><span class="o">=</span><span class="n">ref_logprobs</span><span class="p">,</span>
                                       <span class="n">non_score_reward</span><span class="o">=</span><span class="n">non_score_reward</span><span class="p">,</span> <span class="n">train_stats</span><span class="o">=</span><span class="n">train_stats</span><span class="p">,</span>
                                       <span class="n">kl_coef</span><span class="o">=</span><span class="n">kl_coef</span><span class="p">)</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="n">stats_to_np</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
        <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/ppo/calc_stats&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kl_ctl</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;objective/kl&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">])</span>

        <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/ppo/total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span>
        <span class="n">stats</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">timing</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">stats</span>

    <span class="k">def</span> <span class="nf">batched_forward_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_input</span><span class="p">,</span> <span class="n">gen_len</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculate model outputs in multiple batches.&quot;&quot;&quot;</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
        <span class="n">fbs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;forward_batch_size&#39;</span><span class="p">]</span>
        <span class="n">logprobs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">ref_logprobs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">fbs</span><span class="p">)):</span>
            <span class="n">m_input</span> <span class="o">=</span> <span class="n">model_input</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">fbs</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">fbs</span><span class="p">]</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">m_input</span><span class="p">)</span>
            <span class="n">ref_logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="p">(</span><span class="n">m_input</span><span class="p">)</span>
            
            <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">[:,</span> <span class="o">-</span><span class="n">gen_len</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">logprobs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logprobs_from_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">m_input</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:])[:,</span> <span class="o">-</span><span class="n">gen_len</span><span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">ref_logprobs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logprobs_from_logits</span><span class="p">(</span><span class="n">ref_logits</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">m_input</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:])[:,</span> <span class="o">-</span><span class="n">gen_len</span><span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
   
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">logprobs</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">ref_logprobs</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">train_minibatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">model_input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Train one PPO minibatch&quot;&quot;&quot;</span>
        <span class="n">loss_p</span><span class="p">,</span> <span class="n">loss_v</span><span class="p">,</span> <span class="n">train_stats</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">logprobs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">model_input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_p</span> <span class="o">+</span> <span class="n">loss_v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">train_stats</span>
    
    <span class="k">def</span> <span class="nf">compute_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">ref_logprobs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute per token rewards from scores and KL-penalty.&quot;&quot;&quot;</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="n">logprobs</span> <span class="o">-</span> <span class="n">ref_logprobs</span>
        <span class="n">non_score_reward</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">kl_ctl</span><span class="o">.</span><span class="n">value</span> <span class="o">*</span> <span class="n">kl</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">non_score_reward</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">rewards</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">scores</span>
        <span class="k">return</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">non_score_reward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_ctl</span><span class="o">.</span><span class="n">value</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old_logprobs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">model_input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculate policy and value losses.&quot;&quot;&quot;</span>
        <span class="n">lastgaelam</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">advantages_reversed</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">gen_len</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">gen_len</span><span class="p">)):</span>
            <span class="n">nextvalues</span> <span class="o">=</span> <span class="n">values</span><span class="p">[:,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">gen_len</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="mf">0.0</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">nextvalues</span> <span class="o">-</span> <span class="n">values</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span>
            <span class="n">lastgaelam</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;lam&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">lastgaelam</span>
            <span class="n">advantages_reversed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lastgaelam</span><span class="p">)</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">advantages_reversed</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">returns</span> <span class="o">=</span> <span class="n">advantages</span> <span class="o">+</span> <span class="n">values</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">whiten</span><span class="p">(</span><span class="n">advantages</span><span class="p">)</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">advantages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">vpred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">model_input</span><span class="p">)</span>
        <span class="n">logprob</span> <span class="o">=</span> <span class="n">logprobs_from_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">model_input</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
        
        <span class="c1">#only the generation part of the values/logprobs is needed</span>
        <span class="n">logprob</span><span class="p">,</span> <span class="n">vpred</span> <span class="o">=</span> <span class="n">logprob</span><span class="p">[:,</span> <span class="o">-</span><span class="n">gen_len</span><span class="p">:],</span> <span class="n">vpred</span><span class="p">[:,</span><span class="o">-</span><span class="n">gen_len</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">vpredclipped</span> <span class="o">=</span> <span class="n">clip_by_value</span><span class="p">(</span><span class="n">vpred</span><span class="p">,</span>
                                     <span class="n">values</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s2">&quot;cliprange_value&quot;</span><span class="p">],</span>
                                     <span class="n">values</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s2">&quot;cliprange_value&quot;</span><span class="p">])</span>

        <span class="n">vf_losses1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vpred</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">vf_losses2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vpredclipped</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">vf_loss</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">vf_losses1</span><span class="p">,</span> <span class="n">vf_losses2</span><span class="p">))</span>
        <span class="n">vf_clipfrac</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">vf_losses2</span><span class="p">,</span> <span class="n">vf_losses1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>

        <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprob</span> <span class="o">-</span> <span class="n">old_logprobs</span><span class="p">)</span>
        
        <span class="n">pg_losses</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">ratio</span>
        <span class="n">pg_losses2</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span>
                                               <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;cliprange&#39;</span><span class="p">],</span>
                                               <span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;cliprange&#39;</span><span class="p">])</span>

        <span class="n">pg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pg_losses</span><span class="p">,</span> <span class="n">pg_losses2</span><span class="p">))</span>
        <span class="n">pg_clipfrac</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">pg_losses2</span><span class="p">,</span> <span class="n">pg_losses</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">pg_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;vf_coef&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">vf_loss</span>

        <span class="n">entropy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">entropy_from_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
        <span class="n">approxkl</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">logprob</span> <span class="o">-</span> <span class="n">old_logprobs</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">policykl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logprob</span> <span class="o">-</span> <span class="n">old_logprobs</span><span class="p">)</span>
        <span class="n">return_mean</span><span class="p">,</span> <span class="n">return_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">returns</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
        <span class="n">value_mean</span><span class="p">,</span> <span class="n">value_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

        <span class="n">stats</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">policy</span><span class="o">=</span><span class="n">pg_loss</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">vf_loss</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">loss</span><span class="p">),</span>
            <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">entropy</span><span class="o">=</span><span class="n">entropy</span><span class="p">,</span> <span class="n">approxkl</span><span class="o">=</span><span class="n">approxkl</span><span class="p">,</span><span class="n">policykl</span><span class="o">=</span><span class="n">policykl</span><span class="p">,</span> <span class="n">clipfrac</span><span class="o">=</span><span class="n">pg_clipfrac</span><span class="p">,</span>
                        <span class="n">advantages</span><span class="o">=</span><span class="n">advantages</span><span class="p">,</span> <span class="n">advantages_mean</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">advantages</span><span class="p">),</span> <span class="n">ratio</span><span class="o">=</span><span class="n">ratio</span><span class="p">),</span>
            <span class="n">returns</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">return_mean</span><span class="p">,</span> <span class="n">var</span><span class="o">=</span><span class="n">return_var</span><span class="p">),</span>
            <span class="n">val</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">vpred</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">vpred</span><span class="p">),</span> <span class="n">error</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">vpred</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span>
                     <span class="n">clipfrac</span><span class="o">=</span><span class="n">vf_clipfrac</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">value_mean</span><span class="p">,</span> <span class="n">var</span><span class="o">=</span><span class="n">value_var</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">pg_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ppo_params</span><span class="p">[</span><span class="s1">&#39;vf_coef&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">vf_loss</span><span class="p">,</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">record_step_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kl_coef</span><span class="p">,</span> <span class="o">**</span><span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Record training step statistics.&quot;&quot;&quot;</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;logprobs&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;ref_logprobs&#39;</span><span class="p">]</span>
        <span class="n">mean_kl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">mean_entropy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;logprobs&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">mean_non_score_reward</span> <span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;non_score_reward&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;objective/kl&#39;</span><span class="p">:</span> <span class="n">mean_kl</span><span class="p">,</span>
            <span class="s1">&#39;objective/kl_dist&#39;</span><span class="p">:</span> <span class="n">kl</span><span class="p">,</span>
            <span class="s1">&#39;objective/logprobs&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;logprobs&#39;</span><span class="p">],</span>
            <span class="s1">&#39;objective/ref_logprobs&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;ref_logprobs&#39;</span><span class="p">],</span>
            <span class="s1">&#39;objective/kl_coef&#39;</span><span class="p">:</span> <span class="n">kl_coef</span><span class="p">,</span>
            <span class="s1">&#39;objective/entropy&#39;</span><span class="p">:</span> <span class="n">mean_entropy</span><span class="p">,</span>
            <span class="s1">&#39;ppo/mean_non_score_reward&#39;</span><span class="p">:</span> <span class="n">mean_non_score_reward</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;train_stats&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">stats</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;ppo/</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;ppo/val/var_explained&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;ppo/val/error&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;ppo/returns/var&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">stats</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PPOTrainer" class="doc_header"><code>class</code> <code>PPOTrainer</code><a href="https://github.com/lvwerra/trl/tree/master/trl/ppo.py#L54" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PPOTrainer</code>(<strong><code>model</code></strong>, <strong><code>ref_model</code></strong>, <strong>**<code>ppo_params</code></strong>)</p>
</blockquote>
<p>The PPO_trainer uses Proximal Policy Optimization to optimise language models.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tensor-shapes-and-contents">Tensor shapes and contents<a class="anchor-link" href="#Tensor-shapes-and-contents"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Debugging tensor shapes and contents usually involves inserting a lot of print statements in the code. To avoid this in the future I add a list of the tensor shapes and contents for reference. If the tensors are sliced or reshaped I list the last shape.</p>
<table>
<thead><tr>
<th>Name</th>
<th>Shape</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>query</code></td>
<td><code>[batch_size, query_length]</code></td>
<td>contains token ids of query</td>
</tr>
<tr>
<td><code>response</code></td>
<td><code>[batch_size, response_length]</code></td>
<td>contains token ids of responses</td>
</tr>
<tr>
<td><code>scores</code></td>
<td><code>[batch_size]</code></td>
<td>rewards of each query/response pair</td>
</tr>
<tr>
<td><code>model_input</code></td>
<td><code>[batch_size, query_length + response_length]</code></td>
<td>combined query and response tokens</td>
</tr>
<tr>
<td><code>m_input</code></td>
<td><code>[forward_batch_size, query_length + response_length]</code></td>
<td>small forward batch of model_input</td>
</tr>
<tr>
<td><code>logits</code></td>
<td><code>[forward_batch_size, query_length + response_length, vocab_size]</code></td>
<td>logits from model outputs</td>
</tr>
<tr>
<td><code>ref_logits</code></td>
<td><code>[forward_batch_size, query_length + response_length, vocab_size]</code></td>
<td>logits from ref_model outputs</td>
</tr>
<tr>
<td><code>logprobs</code></td>
<td><code>[batch_size, response_length]</code></td>
<td>log-probabilities of response tokens</td>
</tr>
<tr>
<td><code>ref_logprobs</code></td>
<td><code>[batch_size, response_length]</code></td>
<td>reference log-probabilities of response tokens</td>
</tr>
<tr>
<td><code>rewards</code></td>
<td><code>[batch_size, response_length]</code></td>
<td>the model rewards incl. kl-score for each token</td>
</tr>
<tr>
<td><code>non_score_reward</code></td>
<td><code>[batch_size, response_length]</code></td>
<td>the model kl-score for each token</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-output-alignments">Model output alignments<a class="anchor-link" href="#Model-output-alignments"> </a></h2><p>Some notes on output alignments, since I spent a considerable time debugging this. All model outputs are shifted by 1 to the model inputs. That means that the logits are shifted by one as well as values. For this reason the logits and values are always shifted one step to the left. This also means we don't have logits for the first input element and so we delete the first input token when calculating the softmax, since we don't have logits predictions. The same applies for the values and we shift them by index one to the left.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="KL-divergence">KL-divergence<a class="anchor-link" href="#KL-divergence"> </a></h2><p>One question that came up during the implementation was "Why is the KL-divergence just the difference of the log-probs? Where is the probability in front of the log term?". The answer can be found in Sergey Levine's <a href="http://rll.berkeley.edu/deeprlcourse/docs/week_3_lecture_1_dynamics_learning.pdf">lecture slides</a>: To calculate the KL divergence we calculate the expected value of the log term. The probability usually in front of the log-term comes from that expected value and for a set of trajectories we can simply take the mean over the sampled trajectories.</p>

</div>
</div>
</div>
</div>
 

