# -*- coding: utf-8 -*-
"""Sentiment_Analysis_IMDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DrEGXcATVrZl7WhhwiAbycw05-7cCHrp
"""

# Import necessary libraries
import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load IMDB dataset
def load_imdb_dataset(num_words):
    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)
    return train_data, train_labels, test_data, test_labels

# Preprocess data
def preprocess_data(train_data, test_data, max_length):
    train_data = pad_sequences(train_data, maxlen=max_length)
    test_data = pad_sequences(test_data, maxlen=max_length)
    return train_data, test_data

# Build sentiment analysis model
def build_model(num_words, embedding_dim, max_length):
    model = Sequential([
        Embedding(num_words, embedding_dim),
        GlobalAveragePooling1D(),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Train and evaluate model
def train_and_evaluate(model, train_data, train_labels, test_data, test_labels):
    history = model.fit(train_data, train_labels, epochs=10, validation_data=(test_data, test_labels))
    return history

# Main function
def main():
    num_words = 10000
    max_length = 500
    embedding_dim = 16

    train_data, train_labels, test_data, test_labels = load_imdb_dataset(num_words)
    train_data, test_data = preprocess_data(train_data, test_data, max_length)

    # Split data into training and validation sets
    train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)

    model = build_model(num_words, embedding_dim, max_length)
    history = model.fit(train_data, train_labels, epochs=10, validation_data=(val_data, val_labels))

    test_loss, test_acc = model.evaluate(test_data, test_labels)
    print(f'Test accuracy: {test_acc:.2f}')

    # Plot training and validation accuracy
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()